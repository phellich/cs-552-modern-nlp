{"cells":[{"cell_type":"markdown","id":"ac671356","metadata":{"id":"ac671356"},"source":["#  Exercise 8: In-Context Learning with GPT-3.5"]},{"cell_type":"markdown","id":"7fc96df3","metadata":{"id":"7fc96df3"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;\">\n","\n","## **Exercise Description**\n","- In this exercise, you will investigate in-context learning using OpanAI GPT-3.5 model. This exercise contains two parts.\n","\n","- In the first part, you will investigate in-context learning for classification based on a natural language inference (NLI) task.\n","    \n","- In the second part, you will investigate in-context learning for generation based on a story ending generation (SEG) task.\n","\n","\n","### Table of Contents\n","- **[PART 1: In-Context Learning for Natural Langauge Inference](#1)**\n","    - [1.1 Compare Different Shots](#11)\n","    - [1.2 Effect of Neutral In-Context Examples](#12)\n","    - [1.3 Play with Different Verbalizers](#13)\n","    - [1.4 Add Instructions](#14)\n","- **[PART 2: In-Context Learning for Story Ending Generation](#2)**\n","    - [2.1 Zero-Shot Generation](#21)\n","    - [2.2 Few-Shot Generation](#22)\n","    - [2.3 Add Instructions](#23)\n","\n","</div>"]},{"cell_type":"markdown","id":"ff70873e","metadata":{"id":"ff70873e"},"source":["## Setup Your Environment\n","\n","**Note: the Python version for this exercise is 3.9**, please install the following required packages."]},{"cell_type":"code","source":["# if you are using Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iLBx5JbP0xNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740155337,"user_tz":-120,"elapsed":17844,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"a40cc025-2d9a-481c-e282-52b14864a6e2"},"id":"iLBx5JbP0xNR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# fill in the path where you put the Exercise folder into\n","ROOT_PATH = \"/content/drive/MyDrive/Exercise8/\""],"metadata":{"id":"L52FF8Hn1AWC"},"id":"L52FF8Hn1AWC","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"a9a720ab","metadata":{"scrolled":false,"id":"a9a720ab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740200735,"user_tz":-120,"elapsed":12760,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"9c92e6e6-3b81-419b-8121-34e299d7e6aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy==1.22.4 in /usr/local/lib/python3.9/dist-packages (1.22.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.9/dist-packages (3.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (4.65.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (2022.10.31)\n"]}],"source":["!pip install numpy==1.22.4\n","!pip install tqdm==4.65.0\n","!pip install nltk==3.8.1"]},{"cell_type":"markdown","id":"623e14fd","metadata":{"id":"623e14fd"},"source":["You also need to install our **GPT-3.5 wrapper** to interact with OpenAI GPT-3.5 models for free."]},{"cell_type":"code","execution_count":null,"id":"3d4c8ba5","metadata":{"id":"3d4c8ba5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740214720,"user_tz":-120,"elapsed":8408,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"d762dd09-aeb5-4731-8eae-50c520d80ba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing ./drive/MyDrive/Exercise8/gpt_wrapper-0.0.7-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-wrapper==0.0.7) (2.27.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-wrapper==0.0.7) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-wrapper==0.0.7) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-wrapper==0.0.7) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-wrapper==0.0.7) (2022.12.7)\n","Installing collected packages: gpt-wrapper\n","Successfully installed gpt-wrapper-0.0.7\n"]}],"source":["!pip install {ROOT_PATH}gpt_wrapper-0.0.7-py3-none-any.whl"]},{"cell_type":"markdown","id":"d4d0ca95","metadata":{"id":"d4d0ca95"},"source":["Import the required packages for this exercise, including our GPT-3.5 wrapper."]},{"cell_type":"code","execution_count":null,"id":"8d81dd37","metadata":{"id":"8d81dd37","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740221134,"user_tz":-120,"elapsed":2603,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"f726397f-6cf5-435a-86a5-7d403c8d6423"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import json\n","import numpy as np\n","from tqdm import tqdm\n","import random\n","from copy import deepcopy\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize\n","from nltk.translate.meteor_score import meteor_score\n","\n","import gpt_wrapper\n","from gpt_wrapper.chat import Chat"]},{"cell_type":"markdown","id":"3049b54b","metadata":{"id":"3049b54b"},"source":["To facilitate reproduction, we fix a random seed here."]},{"cell_type":"code","execution_count":null,"id":"acca193e","metadata":{"id":"acca193e"},"outputs":[],"source":["seed = 233"]},{"cell_type":"markdown","id":"721322d2","metadata":{"id":"721322d2"},"source":["Enter the exercise API key to get access to our GPT-3.5 wrapper."]},{"cell_type":"code","execution_count":null,"id":"cd376243","metadata":{"id":"cd376243"},"outputs":[],"source":["gpt_wrapper.api_key = \"a5a244d0-2f56-41d3-ac99-9e5efb0e4079\""]},{"cell_type":"markdown","id":"11e49157","metadata":{"id":"11e49157"},"source":["<a name=\"1\"></a>\n","## **PART 1: In-Context Learning for Natural Language Inference**\n","---\n","\n","In this part, you are going to use the GPT-3.5 model to solve the [natural language inference (NLI)](https://towardsdatascience.com/natural-language-inference-an-overview-57c0eecf6517) task based on in-context learning. For this task, model needs to classify the relation of two given sentences (premise and hypothesis) into three classes: entailment, neutral and contradiction."]},{"cell_type":"markdown","id":"ed303c88","metadata":{"id":"ed303c88"},"source":["Here you can take a glance of the training data used for sampling few-shot in-context examples, and the testing data used to query GPT-3.5 language model for classification (along with the gold answers for evaluation)."]},{"cell_type":"code","execution_count":null,"id":"f8ee990d","metadata":{"id":"f8ee990d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740230765,"user_tz":-120,"elapsed":1067,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"b35316c7-6c25-421a-bce4-8f77b5b4c5a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Samples:\n","I know lawyers are always dreadfully careful.\n","I'm well aware that lawyers are always very careful.\n","Answer: entailment\n","\n","\n","Testing Query:\n","The new rights are nice enough\n","Everyone really likes the newest benefits \n","Answer:\n","\n","\n","Gold Answer:\n","neutral\n"]}],"source":["with open(ROOT_PATH+\"nli_classification/train_classification.json\", \"r\") as f:\n","    train_samples = json.load(f)\n","with open(ROOT_PATH+\"nli_classification/test_classification.json\", \"r\") as f:\n","    test_data = json.load(f)\n","\n","print(\"Training Samples:\")\n","print(train_samples[\"entailment\"][0])\n","print(\"\\n\")\n","\n","print(\"Testing Query:\")\n","print(test_data[0][\"query\"])\n","print(\"\\n\")\n","\n","print(\"Gold Answer:\")\n","print(test_data[0][\"gold_answer\"])"]},{"cell_type":"markdown","id":"01caddef","metadata":{"id":"01caddef"},"source":["Here is the GPT-3.5 hyperparameter setting for this NLI task.\n","\n","**max_tokens**: Maximum number of tokens to generate, default to 16.\n","\n","**temperature**: Sampling temperature to use, between 0.0 and 2.0, default to 1.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n","\n","**top_p**: Nucleus sampling factor (alternative to sampling with temperature), between 0.0 and 1.0, default to 1.0. The model randomly samples from the tokens with top_p probability mass.\n","\n","**presence_penalty**: Between -2.0 and 2.0, default to 0.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n","\n","**frequency_penalty**: Between -2.0 and 2.0, default to 0.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."]},{"cell_type":"markdown","source":["We choose a small *max_tokens* because only the first non-space token generated by the model is used as the predicted class (i.e., verbalizer).\n","\n","We also change the *temparature* to zero in order to let the model make deterministic classification decisions."],"metadata":{"id":"HN3X2CRHdUmW"},"id":"HN3X2CRHdUmW"},{"cell_type":"code","execution_count":null,"id":"ab3bc651","metadata":{"id":"ab3bc651"},"outputs":[],"source":["model_args={\"max_tokens\": 2, \"temperature\": 0.0, \"top_p\": 1.0, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}"]},{"cell_type":"markdown","id":"4ab97e5d","metadata":{"id":"4ab97e5d"},"source":["You will evaluate the model's NLI performance based on the accuracy and F1 scores on each class."]},{"cell_type":"code","execution_count":null,"id":"f6129f62","metadata":{"id":"f6129f62"},"outputs":[],"source":["def evaluate_nli(predictions, gold_labels, mapping):\n","    \n","    counter = np.zeros((3, 3))  # three-class confusion matrix\n","    \n","    # calculate the confusion matrix\n","    for p, g in zip(predictions, gold_labels):\n","        pid = mapping[p]\n","        gid = mapping[g]\n","        counter[gid][pid] += 1\n","    \n","    print()\n","    print(counter)\n","    \n","    pred_sum = np.sum(counter, axis=0)  # total number of predictions on each class\n","    gold_sum = np.sum(counter, axis=1)  # total number of test samples (gold labels) on each class\n","    diag = np.diagonal(counter)  # total number of correct predictions on each class\n","    \n","    acc = np.sum(diag) / np.sum(counter)  # accuracy\n","    \n","    f1 = [0, 0, 0]\n","    for cid in range(3):\n","        precision = diag[cid] / pred_sum[cid]  # precisions on each class\n","        recall = diag[cid] / gold_sum[cid]  # recalls on each class\n","        f1[cid] = 2 * precision * recall / (precision + recall)  # F1 scores on each class\n","    \n","    return acc, f1[0], f1[1], f1[2]"]},{"cell_type":"markdown","id":"d886b1c6","metadata":{"id":"d886b1c6"},"source":["You will use the following function to perform GPT-3.5 inference on the NLI task based on in-context learning."]},{"cell_type":"code","execution_count":null,"id":"7a878781","metadata":{"id":"7a878781"},"outputs":[],"source":["def gpt3_nli(train_samples, test_data, shots, predictions, gold_answers,\n","             introduction=None, default_class=\"neutral\", task_name=\"none\"):\n","    \n","    '''\n","    train_samples: training data for sampling in-context examples\n","    train_data: testing queries (with gold labels)\n","    shots: number of in-context examples (shots) per class\n","    predictions: cache for saving the model predictions\n","    gold_answers: cache for saving gold answers\n","    introduction: additional task introduction for prompting\n","    default_class: default prediction class if the generated token is not among the verbalizers of three NLI classes\n","    task_name: task name for creating chat sessions\n","    '''\n","    \n","    # randomly sample in-context examples\n","    examples = []\n","    for nli_class, samples in train_samples.items():\n","        few_shot_samples = random.sample(samples, shots[nli_class])\n","        examples.extend(few_shot_samples)\n","\n","    random.shuffle(examples)  # randomly shuffle sampled in-context examples\n","    \n","    # add task introduction (if it exists) before in-context examples for better prompting\n","    if introduction:\n","        examples.insert(0, introduction)\n","\n","    for qid, query in enumerate(tqdm(test_data)):\n","        \n","        if qid < len(predictions):  # skip this query if its model prediction is already saved in cache\n","            continue\n","\n","        # concatenate all the in-context examples with the query, to get the final input demonstration\n","        demonstration = \"\\n\\n\".join(examples+[query[\"query\"]])\n","\n","        # create a chat session using our GPT-3.5 wrapper class Chat\n","        chat = Chat.create(name=task_name+\"_\"+str(qid))\n","        \n","        # use the created chat session to query the GPT-3.5 model with the input demonstration,\n","        # and get back model's output message\n","        message = chat.ask(demonstration, model_args=model_args)\n","        \n","        # model's output text is in the attribute \"content\",\n","        # we use the first token of the generated text as the prediction\n","        preds = message.content.strip().split()\n","        if preds:\n","            pred = preds[0].lower()\n","        else:\n","            pred = \"none\"\n","        \n","        # mapping similar outputs to class verbalizers\n","        if pred in [\"entail\", \"entailed\", \"entailing\"]:\n","            pred = \"entailment\"\n","        if pred in [\"contrad\", \"contradict\", \"contradicted\", \"contradicting\"]:\n","            pred = \"contradiction\"\n","        \n","        # save the prediction in chace\n","        if pred in train_samples.keys():\n","            predictions.append(pred)\n","        else:\n","            predictions.append(default_class)\n","        \n","        # save the gold answer in chace for evaluation\n","        gold_answers.append(query[\"gold_answer\"])"]},{"cell_type":"markdown","id":"a6619024","metadata":{"id":"a6619024"},"source":["You will run the following function to perform GPT-3.5 inference and evaluation."]},{"cell_type":"code","execution_count":null,"id":"692af4be","metadata":{"id":"692af4be"},"outputs":[],"source":["def run(train_samples, test_data, class_shots, mapping, predictions, gold_answers,\n","        introduction=None, default_class=\"neutral\", task_name=\"none\"):\n","    try:\n","\n","        gpt3_nli(train_samples, test_data, class_shots, predictions, gold_answers,\n","                 introduction=introduction, default_class=default_class, task_name=task_name)\n","        \n","        acc, f1_ent, f1_neu, f1_con = evaluate_nli(predictions, gold_answers, mapping)\n","        macro_f1 = (f1_ent + f1_neu + f1_con) / 3\n","\n","        print(f'Accuracy: {acc*100:.2f}% | F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')\n","\n","    except Exception as error:  # OpenAI ChatGPT endpoint (gpt-3.5-turbo) may get stucked by too many queries from time to time\n","    \n","        print(error)\n"]},{"cell_type":"markdown","id":"c90ea96b","metadata":{"id":"c90ea96b"},"source":["<a name=\"11\"></a>\n","### **1.1 Compare Different Shots**\n","\n","In this part, you will compare GPT-3.5 performances under different shots (number) of in-context examples."]},{"cell_type":"markdown","id":"2f25d6a1","metadata":{"id":"2f25d6a1"},"source":["#### 0-shot classification:\n","\n","Do not provide any in-context learning examples to the model.\n","\n","Create empty caches for saving model predictions and gold answers."]},{"cell_type":"code","execution_count":null,"id":"0ae867da","metadata":{"id":"0ae867da"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"markdown","id":"3921da49","metadata":{"id":"3921da49"},"source":["Run the inferece and evaluation.\n","\n","**Note:** OpenAI ChatGPT endpoint may sometimes get stucked by too many queries. If running the following cell gets stucked, just re-run it, and inference will continue from the stucked query. However, do not re-run the above cell for creating the caches, which will clear the already saved predictions."]},{"cell_type":"code","execution_count":null,"id":"78fa56d7","metadata":{"id":"78fa56d7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740282153,"user_tz":-120,"elapsed":22585,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"1eea57e7-128d-4d68-e877-b3910de50084"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.32it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[ 0. 10.  0.]\n"," [ 0. 10.  0.]\n"," [ 0. 10.  0.]]\n","Accuracy: 33.33% | F1: (nan%, 50.00%, nan%) | Macro-F1: nan%\n"]},{"output_type":"stream","name":"stderr","text":["\n","<ipython-input-10-116f85ef2836>:22: RuntimeWarning: invalid value encountered in double_scalars\n","  precision = diag[cid] / pred_sum[cid]  # precisions on each class\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots = {\"entailment\": 0, \"neutral\": 0, \"contradiction\": 0}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots0\")"]},{"cell_type":"markdown","id":"158536b5","metadata":{"id":"158536b5"},"source":["#### 1-shot per class:\n","\n","For each class, provide 1 in-context learning example sampled from the training data.\n","\n","Clear the caches."]},{"cell_type":"code","execution_count":null,"id":"6bd54af4","metadata":{"id":"6bd54af4"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"markdown","id":"a43cb520","metadata":{"id":"a43cb520"},"source":["Re-run the inference and evaluation"]},{"cell_type":"code","execution_count":null,"id":"f544c0fd","metadata":{"id":"f544c0fd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740311281,"user_tz":-120,"elapsed":23085,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"655c95a4-1be4-45f0-d519-8fda494f85a1"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[6. 3. 1.]\n"," [5. 2. 3.]\n"," [0. 1. 9.]]\n","Accuracy: 56.67% | F1: (57.14%, 25.00%, 78.26%) | Macro-F1: 53.47%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots1\")"]},{"cell_type":"markdown","id":"1f2eb071","metadata":{"id":"1f2eb071"},"source":["#### 2-shot per class:\n","\n","Try 2 in-context learning examples per class."]},{"cell_type":"code","execution_count":null,"id":"3ae49ee1","metadata":{"id":"3ae49ee1"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"143c7eba","metadata":{"id":"143c7eba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740340798,"user_tz":-120,"elapsed":23763,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"12d893fb-401c-433a-fd47-b77128db9275"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[6. 3. 1.]\n"," [4. 3. 3.]\n"," [0. 2. 8.]]\n","Accuracy: 56.67% | F1: (60.00%, 33.33%, 72.73%) | Macro-F1: 55.35%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots = {\"entailment\": 2, \"neutral\": 2, \"contradiction\": 2}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots2\")"]},{"cell_type":"markdown","id":"ae6f41c3","metadata":{"id":"ae6f41c3"},"source":["#### 3-shot per class:\n","\n","Try 3 in-context learning examples per class."]},{"cell_type":"code","execution_count":null,"id":"c86dc6c2","metadata":{"id":"c86dc6c2"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"32278700","metadata":{"id":"32278700","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740372517,"user_tz":-120,"elapsed":22961,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"a8c0c8e6-ccb2-4912-d3d7-672f0ebe05a9"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[7. 3. 0.]\n"," [4. 1. 5.]\n"," [1. 0. 9.]]\n","Accuracy: 56.67% | F1: (63.64%, 14.29%, 75.00%) | Macro-F1: 50.97%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots = {\"entailment\": 3, \"neutral\": 3, \"contradiction\": 3}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_1_shots3\")"]},{"cell_type":"markdown","id":"fcca3308","metadata":{"id":"fcca3308"},"source":["**Questions:**\n","\n","1. Can model handle well the NLI task without in-context examples for learning (i.e., under the 0-shot setting)?\n","2. On detecting which class are the in-context examples most helpful? and most helpless?\n","3. Is the more in-context examples the better?"]},{"cell_type":"markdown","id":"80e8fab3","metadata":{"id":"80e8fab3"},"source":["**Reference Answers:**\n","\n","1. No, because the model cannot learn to generate the required verbalizers (i.e., entailment, neutral and contradiction) during the classification, so the predictions are always the default class.\n","2. In-context examples are most helpful in detecting the contradiction class, while most helpless in detecting the neutral class, probably because neutral samples are more prone to be identified as having some entailed or contradicted relations.\n","3. No, more in-context examples does not necessarily lead to better performance."]},{"cell_type":"markdown","id":"ae87231e","metadata":{"id":"ae87231e"},"source":["<a name=\"12\"></a>\n","### **1.2 Effect of Neutral In-Context Examples**\n","\n","Try 3-shot in-context examples on the entailment and contradictions classes, but do not provide any examples on the neutral class."]},{"cell_type":"code","execution_count":null,"id":"f6d78c89","metadata":{"id":"f6d78c89"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"ae734042","metadata":{"id":"ae734042","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740411286,"user_tz":-120,"elapsed":23782,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"16e483c8-122e-48bb-bd26-74d371cb136f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:23<00:00,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[8. 1. 1.]\n"," [3. 2. 5.]\n"," [1. 0. 9.]]\n","Accuracy: 63.33% | F1: (72.73%, 30.77%, 72.00%) | Macro-F1: 58.50%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots = {\"entailment\": 3, \"neutral\": 0, \"contradiction\": 3}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping, predictions, gold_answers, task_name=\"1_2\")"]},{"cell_type":"markdown","id":"e8ef6a43","metadata":{"id":"e8ef6a43"},"source":["**Question:** What do you find here?"]},{"cell_type":"markdown","id":"8c1342c4","metadata":{"id":"8c1342c4"},"source":["**Reference Answer:** The model's performance on detecting entailment queries is improved, while this does not change the model's performance on detecting neutral queries. So neutral in-context examples may not be that effective in helping detect the neutral queries in this task, which may also cause confusion on detecting the entailment queries."]},{"cell_type":"markdown","id":"77c56bf9","metadata":{"id":"77c56bf9"},"source":["<a name=\"13\"></a>\n","### **1.3 Play with Different Verbalizers**\n","\n","In this part, you will try to use different verbalizers for this NLI classification task. Instead of using *entailment*, *neutral* and *contradiction*, you will try the following two alternatives:\n","\n","- *positive*, *unrelated* and *negative*\n","- *a*, *b* and *c*"]},{"cell_type":"markdown","id":"f9f7e4f2","metadata":{"id":"f9f7e4f2"},"source":["Build data with the above two different verbalizers."]},{"cell_type":"code","execution_count":null,"id":"88f33a23","metadata":{"id":"88f33a23"},"outputs":[],"source":["mapping_to_pun = {\"entailment\": \"positive\", \"neutral\": \"unrelated\", \"contradiction\": \"negative\"}\n","train_samples_pun = {\"positive\": [], \"unrelated\": [], \"negative\": []}\n","test_data_pun = []\n","\n","mapping_to_abc = {\"entailment\": \"a\", \"neutral\": \"b\", \"contradiction\": \"c\"}\n","train_samples_abc = {\"a\": [], \"b\": [], \"c\": []}\n","test_data_abc = []\n","\n","for nli_class, samples in train_samples.items():\n","    \n","    nli_class_pun = mapping_to_pun[nli_class]\n","    nli_class_abc = mapping_to_abc[nli_class]\n","    \n","    for sample in samples:\n","        \n","        sample_pun = \" \".join(sample.split(\" \")[:-1] + [nli_class_pun])\n","        train_samples_pun[nli_class_pun].append(sample_pun)\n","        \n","        sample_abc = \" \".join(sample.split(\" \")[:-1] + [nli_class_abc])\n","        train_samples_abc[nli_class_abc].append(sample_abc)\n","    \n","for query in test_data:\n","    \n","    query_pun = deepcopy(query)\n","    query_pun[\"gold_answer\"] = mapping_to_pun[query[\"gold_answer\"]]\n","    test_data_pun.append(query_pun)\n","    \n","    query_abc = deepcopy(query)\n","    query_abc[\"gold_answer\"] = mapping_to_abc[query[\"gold_answer\"]]\n","    test_data_abc.append(query_abc)"]},{"cell_type":"markdown","id":"fc11b99e","metadata":{"id":"fc11b99e"},"source":["You can take a glance of the processed training and testing data with different verbalizers.\n","\n","Data with verbalizers *positive*, *unrelated* and *negative*"]},{"cell_type":"code","execution_count":null,"id":"7ae7c0de","metadata":{"id":"7ae7c0de","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740449664,"user_tz":-120,"elapsed":665,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"29004591-1864-44f3-e250-6bbbbba1cd03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Samples:\n","I know lawyers are always dreadfully careful.\n","I'm well aware that lawyers are always very careful.\n","Answer: positive\n","\n","\n","Testing Query:\n","The new rights are nice enough\n","Everyone really likes the newest benefits \n","Answer:\n","\n","\n","Gold Answer:\n","unrelated\n"]}],"source":["print(\"Training Samples:\")\n","print(train_samples_pun[\"positive\"][0])\n","print(\"\\n\")\n","\n","print(\"Testing Query:\")\n","print(test_data_pun[0][\"query\"])\n","print(\"\\n\")\n","\n","print(\"Gold Answer:\")\n","print(test_data_pun[0][\"gold_answer\"])"]},{"cell_type":"markdown","id":"a4783fca","metadata":{"id":"a4783fca"},"source":["Data with verbalizers *a*, *b* and *c*"]},{"cell_type":"code","execution_count":null,"id":"2f64c3cb","metadata":{"id":"2f64c3cb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740451809,"user_tz":-120,"elapsed":2,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"39cfc7d3-453f-4b17-d6dd-9faca32ff8f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Samples:\n","I know lawyers are always dreadfully careful.\n","I'm well aware that lawyers are always very careful.\n","Answer: a\n","\n","\n","Testing Query:\n","The new rights are nice enough\n","Everyone really likes the newest benefits \n","Answer:\n","\n","\n","Gold Answer:\n","b\n"]}],"source":["print(\"Training Samples:\")\n","print(train_samples_abc[\"a\"][0])\n","print(\"\\n\")\n","\n","print(\"Testing Query:\")\n","print(test_data_abc[0][\"query\"])\n","print(\"\\n\")\n","\n","print(\"Gold Answer:\")\n","print(test_data_abc[0][\"gold_answer\"])"]},{"cell_type":"markdown","id":"eb30fd47","metadata":{"id":"eb30fd47"},"source":["#### Re-do the classification with new verbalizers.\n","\n","Try verbalizers *positive*, *unrelated* and *negative* under the 2-shot setting in 1.1"]},{"cell_type":"code","execution_count":null,"id":"14b52710","metadata":{"id":"14b52710"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"3842ede7","metadata":{"id":"3842ede7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740485335,"user_tz":-120,"elapsed":23845,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"c83cffae-6cfd-4d6d-9894-eca326ba7e12"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[9. 1. 0.]\n"," [5. 3. 2.]\n"," [1. 0. 9.]]\n","Accuracy: 70.00% | F1: (72.00%, 42.86%, 85.71%) | Macro-F1: 66.86%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots_pun = {\"positive\": 2, \"unrelated\": 2, \"negative\": 2}\n","mapping_pun = {\"positive\": 0, \"unrelated\": 1, \"negative\": 2}\n","\n","run(train_samples_pun, test_data_pun, class_shots_pun, mapping_pun,\n","    predictions, gold_answers, default_class=\"unrelated\", task_name=\"1_3_pun\")"]},{"cell_type":"markdown","id":"5ada009a","metadata":{"id":"5ada009a"},"source":["Try verbalizers *a*, *b* and *c* under the 2-shot setting in 1.1"]},{"cell_type":"code","execution_count":null,"id":"f1efccae","metadata":{"id":"f1efccae"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"df8194f6","metadata":{"id":"df8194f6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740556153,"user_tz":-120,"elapsed":9311,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"d90bc77a-43cb-42b3-ab1b-df2daf9ccedf"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:08<00:00,  3.36it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[4. 6. 0.]\n"," [2. 6. 2.]\n"," [0. 7. 3.]]\n","Accuracy: 43.33% | F1: (50.00%, 41.38%, 40.00%) | Macro-F1: 43.79%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","class_shots_abc = {\"a\": 2, \"b\": 2, \"c\": 2}\n","mapping_abc = {\"a\": 0, \"b\": 1, \"c\": 2}\n","\n","run(train_samples_abc, test_data_abc, class_shots_abc, mapping_abc,\n","    predictions, gold_answers, default_class=\"b\", task_name=\"1_3_abc\")"]},{"cell_type":"markdown","id":"2c6f384d","metadata":{"id":"2c6f384d"},"source":["**Questions:**\n","\n","1. Are verbalizers *positive*, *unrelated* and *negative* better or worse than the original ones?\n","2. Are verbalizers *a*, *b* and *c* better or worse than the original ones?"]},{"cell_type":"markdown","id":"13df3e9b","metadata":{"id":"13df3e9b"},"source":["**Reference Answers:**\n","\n","1. They are better because \"unrelated\" could be more easily distinguished from \"positive\" and \"negative\", compared with \"neutral\" to \"entailment\" and \"contradiction\". So in-context examples of the \"unrelated\" class cause less confusion on the detection of \"positive\" and \"negative\" classes.\n","2. They are worse because \"a\", \"b\" and \"c\" do not have the meanings that are correlated with the \"entailment\", \"neutral\" and \"contradiction\", so the model does not learn well on distinguishing these three classes."]},{"cell_type":"markdown","id":"77c62887","metadata":{"id":"77c62887"},"source":["<a name=\"14\"></a>\n","### **1.4 Add Instructions**\n","\n","In this part, you will try to add high-level task instruction to the model input.\n","\n","Try 1-shot in-context learning with overall task introduction: \"Guess whether the given two sentences have an entailment, neutral or contradiction relation.\""]},{"cell_type":"code","execution_count":null,"id":"f123b9cb","metadata":{"id":"f123b9cb"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"8252537f","metadata":{"id":"8252537f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740593777,"user_tz":-120,"elapsed":23221,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"6ca69e25-0d69-4b88-ec62-9b13fa9dc992"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:23<00:00,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[8. 1. 1.]\n"," [4. 3. 3.]\n"," [1. 0. 9.]]\n","Accuracy: 66.67% | F1: (69.57%, 42.86%, 78.26%) | Macro-F1: 63.56%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","introduction = \"Guess whether the given two sentences have an entailment, neutral or contradiction relation.\"\n","class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping,\n","    predictions, gold_answers, introduction=introduction, task_name=\"1_4_intro1\")"]},{"cell_type":"markdown","id":"febb96f6","metadata":{"id":"febb96f6"},"source":["Try to use a more specified task introduction as the instruction: \"Guess whether the second statement is entailed by the first statement, contradicts the first statement, or is neutral to the first statement.\""]},{"cell_type":"code","execution_count":null,"id":"312ff74a","metadata":{"id":"312ff74a"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"ffb6580f","metadata":{"id":"ffb6580f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740623017,"user_tz":-120,"elapsed":21934,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"632bf53f-92f6-4804-9be1-9b258c75d3d7"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[10.  0.  0.]\n"," [ 5.  3.  2.]\n"," [ 1.  1.  8.]]\n","Accuracy: 70.00% | F1: (76.92%, 42.86%, 80.00%) | Macro-F1: 66.59%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","introduction = \"Guess whether the second statement is entailed by the first statement, contradicts the first statement, or is neutral to the first statement.\"\n","class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping,\n","    predictions, gold_answers, introduction=introduction, task_name=\"1_4_intro2\")"]},{"cell_type":"markdown","id":"8fed052d","metadata":{"id":"8fed052d"},"source":["Try to make the model think that he is an expert on doing this task! Use the instruction: \"Pretend that you are an expert of logic. Tell us whether the second statement is entailed by the first statement, contradicts the first statement, or is neutral to the first statement.\""]},{"cell_type":"code","execution_count":null,"id":"fbd84edc","metadata":{"id":"fbd84edc"},"outputs":[],"source":["predictions = []\n","gold_answers = []"]},{"cell_type":"code","execution_count":null,"id":"fb12f254","metadata":{"id":"fb12f254","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740686824,"user_tz":-120,"elapsed":23325,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"af0846d3-06fe-4da2-ee98-4fbe2620fc40"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:22<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","[[9. 1. 0.]\n"," [4. 5. 1.]\n"," [1. 2. 7.]]\n","Accuracy: 70.00% | F1: (75.00%, 55.56%, 77.78%) | Macro-F1: 69.44%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","introduction = \"Pretend that you are an expert of logic. Tell us whether the second statement is entailed by the first statement, contradicts the first statement, or is neutral to the first statement.\"\n","class_shots = {\"entailment\": 1, \"neutral\": 1, \"contradiction\": 1}\n","mapping = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n","\n","run(train_samples, test_data, class_shots, mapping,\n","    predictions, gold_answers, introduction=introduction, task_name=\"1_4_intro3\")"]},{"cell_type":"markdown","id":"f17d0159","metadata":{"id":"f17d0159"},"source":["**Question:** Does additional task instruction help? Does more specific insturction tend to be better? Is it effective to give the model a role (e.g., expert) before doing the task?"]},{"cell_type":"markdown","id":"736080fa","metadata":{"id":"736080fa"},"source":["**Reference Answer:** GPT-3.5 is tuned to be very good at following instructions. So adding task instructions at the beginning of the input is often effective to improve the model perfomance, especially under very low-shot settings. More specific insturction can often lead to better performance, since richer context helps the model learn more about the task. Interestingly, giving the model a role-play setting related to the task can often also achieve improvements."]},{"cell_type":"markdown","id":"5c8f6131","metadata":{"id":"5c8f6131"},"source":["<a name=\"2\"></a>\n","## **PART 2: In-Context Learning for Story Ending Generation**\n","---\n","\n","In this part, you will switch to using the GPT-3.5 model to solve the story ending generation (SEG) task based on in-context learning. For this task, model is given four lines of story plot and needs to generate the fifth line of the story plot as an ending."]},{"cell_type":"markdown","id":"2f52dacd","metadata":{"id":"2f52dacd"},"source":["You can take a glance of the training data used for sampling few-shot in-context examples, and the testing data used to query GPT-3.5 language model for story completion (along with the reference story ending)."]},{"cell_type":"code","execution_count":null,"id":"2d31a9bd","metadata":{"id":"2d31a9bd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681740778880,"user_tz":-120,"elapsed":1480,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"9fa6d309-b0d7-4f2f-8f73-5647ff995391"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Samples:\n","Dan's parents were overweight.\n","Dan was overweight as well.\n","The doctors told his parents it was unhealthy.\n","His parents understood and decided to make a change.\n","Output: They got themselves and Dan on a diet.\n","\n","\n","Testing Query:\n","A few days ago I decided to take my dog Sable for a walk.\n","She is a half-pit bull half-bulldog with a lot of strength.\n","After I got her leash on I opened the garage to head outside.\n","She tried bolting out of the garage and dragged me along with her.\n","Output:\n","\n","\n","Reference Story Ending:\n","After the half hour walk my muscles were sore as if I had worked out.\n"]}],"source":["with open(ROOT_PATH+\"story_generation/train_generation.json\", \"r\") as f:\n","    train_samples_sg = json.load(f)\n","with open(ROOT_PATH+\"story_generation/test_generation.json\", \"r\") as f:\n","    test_data_sg = json.load(f)\n","\n","print(\"Training Samples:\")\n","print(train_samples_sg[0])\n","print(\"\\n\")\n","\n","print(\"Testing Query:\")\n","print(test_data_sg[0][\"query\"])\n","print(\"\\n\")\n","\n","print(\"Reference Story Ending:\")\n","print(test_data_sg[0][\"reference_ending\"])"]},{"cell_type":"markdown","id":"a8a56277","metadata":{"id":"a8a56277"},"source":["Here is the GPT-3.5 hyperparameter setting for this SEG task.\n","\n","We set *max_tokens* to be 20, which is supposed to be the maximum length of a story ending (i.e., a sentence).\n","\n","We also change the *temparature* and *top_p* to 0.9 in order to enable the model's creativity and make it generates more diverse story endings."]},{"cell_type":"code","execution_count":null,"id":"489af8bb","metadata":{"id":"489af8bb"},"outputs":[],"source":["model_args={\"max_tokens\": 20, \"temperature\": 0.9, \"top_p\": 0.9, \"presence_penalty\": 0.0, \"frequency_penalty\": 0.0}"]},{"cell_type":"markdown","id":"d59fcc7f","metadata":{"id":"d59fcc7f"},"source":["You will evaluate the model generation performance based on [METEOR](https://aclanthology.org/W05-0909.pdf). This metric is originally proposed to evaluate machine translation quality, but later widely used in evaluating open-domain text (e.g., dialogues and stories) generation. It measures the alignments (i.e., matches) between words in the hypothesis to reference, by sequentially applying exact match, stemmed match and wordnet based synonym match."]},{"cell_type":"code","execution_count":null,"id":"93f4a599","metadata":{"id":"93f4a599"},"outputs":[],"source":["def evaluate_sed(generation, reference):\n","    \n","    ref_tokens = word_tokenize(reference)\n","    gen_tokens = word_tokenize(generation)\n","    score = meteor_score([ref_tokens], gen_tokens)\n","    \n","    return score"]},{"cell_type":"markdown","id":"ff69d0d4","metadata":{"id":"ff69d0d4"},"source":["You will use the following function to perform GPT-3.5 generation on the SEG task based on in-context learning."]},{"cell_type":"code","execution_count":null,"id":"352adb17","metadata":{"id":"352adb17"},"outputs":[],"source":["def gpt3_seg(train_samples, test_data, shot, generations, queries, reference_endings,\n","             introduction=None, task_name=\"none\"):\n","\n","    '''\n","    train_samples: training data for sampling in-context examples\n","    train_data: testing queries (with reference story endings)\n","    shot: number of in-context examples\n","    generations: cache for saving the model generations\n","    queries: cache for saving the input queries (i.e., four-line stories to be completed)\n","    reference_endings: cache for reference story endings\n","    introduction: additional task introduction for prompting\n","    task_name: task name for creating chat sessions\n","    '''\n","    \n","    # randomly sample in-context examples and shuffle them\n","    examples = random.sample(train_samples, shot)\n","    random.shuffle(examples)\n","    \n","    # add task introduction (if it exists) before in-context examples for better prompting\n","    if introduction:\n","        examples.insert(0, introduction)\n","\n","    for qid, query in enumerate(tqdm(test_data)):\n","        \n","        if qid < len(generations):  # skip this query if its model generated story ending is already saved in cache\n","            continue\n","\n","        # concatenate all the in-context examples with the query, to get the final input demonstration\n","        demonstration = \"\\n\\n\".join(examples+[query[\"query\"]])\n","\n","        # create a chat session using our GPT-3.5 wrapper and query the model to get the story ending generation\n","        chat = Chat.create(name=task_name+\"_\"+str(qid))\n","        message = chat.ask(demonstration, model_args=model_args)\n","        \n","        # save the model generation, story query and reference ending in caches\n","        generations.append(message.content)\n","        queries.append(query[\"query\"])\n","        reference_endings.append(query[\"reference_ending\"])"]},{"cell_type":"markdown","id":"5b28953b","metadata":{"id":"5b28953b"},"source":["You will run the following function to perform GPT-3.5 generation and evaluation."]},{"cell_type":"code","execution_count":null,"id":"fe1e696a","metadata":{"id":"fe1e696a"},"outputs":[],"source":["def run(train_samples, test_data, shot, generations, queries, reference_endings, introduction=None, task_name=\"none\"):\n","    \n","    try:\n","        \n","        gpt3_seg(train_samples, test_data, shot,\n","                 generations, queries, reference_endings,\n","                 introduction=introduction, task_name=task_name)\n","\n","        meteor_scores = []\n","        print()\n","\n","        for qid, query in enumerate(queries):\n","\n","            meteor = evaluate_sed(generations[qid], reference_endings[qid])\n","            print(\"Query \"+str(qid+1)+f' METEOR Score: {meteor*100:.2f}') \n","\n","            meteor_scores.append(meteor)\n","\n","        meteor_avg = sum(meteor_scores)/len(meteor_scores)\n","        print(f'Average METEOR Score: {meteor_avg*100:.2f}')\n","    \n","    except Exception as error:  # OpenAI ChatGPT endpoint (gpt-3.5-turbo) may get stucked by too many queries from time to time\n","        \n","        print(error)\n"]},{"cell_type":"markdown","id":"2aabe9f8","metadata":{"id":"2aabe9f8"},"source":["<a name=\"21\"></a>\n","### **2.1 Zero-Shot Generation**\n","\n","Try 0-shot story ending generation (i.e., without any in-context examples).\n","\n","Create caches for saving model predictions, queries and reference story endings."]},{"cell_type":"code","execution_count":null,"id":"ff383572","metadata":{"id":"ff383572"},"outputs":[],"source":["generations_21 = []\n","queries_21 = []\n","reference_endings_21 = []"]},{"cell_type":"markdown","id":"c58400cf","metadata":{"id":"c58400cf"},"source":["Run the generation and evaluation.\n","\n","**Note:** Similar to Part 1, re-run the cell if it gets stucked."]},{"cell_type":"code","execution_count":null,"id":"bc252e8e","metadata":{"id":"bc252e8e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681741289721,"user_tz":-120,"elapsed":16075,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"b72dc55f-e85a-4b7d-d1ea-195dcd8d6e69"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:13<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Query 1 METEOR Score: 12.27\n","Query 2 METEOR Score: 51.74\n","Query 3 METEOR Score: 4.55\n","Query 4 METEOR Score: 0.00\n","Query 5 METEOR Score: 45.43\n","Query 6 METEOR Score: 21.74\n","Query 7 METEOR Score: 36.13\n","Query 8 METEOR Score: 25.66\n","Query 9 METEOR Score: 18.18\n","Query 10 METEOR Score: 9.17\n","Average METEOR Score: 22.49\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","run(train_samples_sg, test_data_sg, 0, generations_21, queries_21, reference_endings_21, task_name=\"2_1_shots0\")"]},{"cell_type":"markdown","id":"028cff13","metadata":{"id":"028cff13"},"source":["You can print the saved caches and compare the quality of the reference and model-generated story endings."]},{"cell_type":"code","execution_count":null,"id":"6e0b2dd1","metadata":{"id":"6e0b2dd1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681741384000,"user_tz":-120,"elapsed":693,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"be324930-dd77-4254-8b2c-c4e8c7391a7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Queries:\n","A few days ago I decided to take my dog Sable for a walk.\n","She is a half-pit bull half-bulldog with a lot of strength.\n","After I got her leash on I opened the garage to head outside.\n","She tried bolting out of the garage and dragged me along with her.\n","Output:\n","GPT-3.5 Generation:\n","A few days ago, as I was taking my dog Sable for a walk, I quickly realized\n","Reference:\n","After the half hour walk my muscles were sore as if I had worked out.\n"]}],"source":["print(\"Queries:\\n\"+queries_21[0])\n","print(\"GPT-3.5 Generation:\\n\"+generations_21[0])\n","print(\"Reference:\\n\"+reference_endings_21[0])"]},{"cell_type":"markdown","id":"ae4813db","metadata":{"id":"ae4813db"},"source":["<a name=\"22\"></a>\n","### **2.2 Few-Shot Generation**\n","\n","Try adding 5-shot in-context examples for this generation task."]},{"cell_type":"code","execution_count":null,"id":"89a6f67a","metadata":{"id":"89a6f67a"},"outputs":[],"source":["generations_22 = []\n","queries_22 = []\n","reference_endings_22 = []"]},{"cell_type":"code","execution_count":null,"id":"5267b3c5","metadata":{"id":"5267b3c5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681741422976,"user_tz":-120,"elapsed":12302,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"d0486b7b-7335-426a-979b-07db5765220a"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:12<00:00,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Query 1 METEOR Score: 6.49\n","Query 2 METEOR Score: 63.05\n","Query 3 METEOR Score: 9.80\n","Query 4 METEOR Score: 60.50\n","Query 5 METEOR Score: 46.54\n","Query 6 METEOR Score: 61.58\n","Query 7 METEOR Score: 23.45\n","Query 8 METEOR Score: 30.68\n","Query 9 METEOR Score: 10.00\n","Query 10 METEOR Score: 14.71\n","Average METEOR Score: 32.68\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","run(train_samples_sg, test_data_sg, 5, generations_22, queries_22, reference_endings_22, task_name=\"2_2_shots5\")"]},{"cell_type":"markdown","id":"44be2268","metadata":{"id":"44be2268"},"source":["**Question:** Do few-shot examples improve the model's story ending generation quality?"]},{"cell_type":"markdown","id":"5ecf3489","metadata":{"id":"5ecf3489"},"source":["**Reference Answer:** According to the automatic evaluations, yes, but such evaluations (based on surface-form matchings with the references) may not be correlated with human judgements. You are encourged to print out model generations in 2.1 and 2.2, and manually make further comparisons to see if there are truly improvements."]},{"cell_type":"markdown","id":"c9bd1485","metadata":{"id":"c9bd1485"},"source":["You can print the saved caches and make more comparisons between the model generations in 2.1 and 2.2."]},{"cell_type":"code","execution_count":null,"id":"e028b9a4","metadata":{"id":"e028b9a4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681741525710,"user_tz":-120,"elapsed":5,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"1730a155-1fa6-47c9-8b9f-97a5b3a67edb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Queries:\n","A few days ago I decided to take my dog Sable for a walk.\n","She is a half-pit bull half-bulldog with a lot of strength.\n","After I got her leash on I opened the garage to head outside.\n","She tried bolting out of the garage and dragged me along with her.\n","Output:\n","GPT-3.5 Generation:\n","I learned to hold onto her leash more securely.\n","Reference:\n","After the half hour walk my muscles were sore as if I had worked out.\n"]}],"source":["print(\"Queries:\\n\"+queries_22[0])\n","print(\"GPT-3.5 Generation:\\n\"+generations_22[0])\n","print(\"Reference:\\n\"+reference_endings_22[0])"]},{"cell_type":"markdown","id":"2276b631","metadata":{"id":"2276b631"},"source":["<a name=\"23\"></a>\n","### **2.3 Add Instructions**\n","\n","Try 0-shot in-context learning with overall task introduction."]},{"cell_type":"code","execution_count":null,"id":"e7bcd744","metadata":{"id":"e7bcd744"},"outputs":[],"source":["generations_23 = []\n","queries_23 = []\n","reference_endings_23 = []"]},{"cell_type":"code","execution_count":null,"id":"efb203a4","metadata":{"id":"efb203a4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681741562163,"user_tz":-120,"elapsed":13668,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"5ffac98a-d961-460a-e22c-165f08000f7b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:13<00:00,  1.33s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Query 1 METEOR Score: 15.34\n","Query 2 METEOR Score: 30.77\n","Query 3 METEOR Score: 29.22\n","Query 4 METEOR Score: 21.62\n","Query 5 METEOR Score: 19.69\n","Query 6 METEOR Score: 61.81\n","Query 7 METEOR Score: 38.12\n","Query 8 METEOR Score: 17.62\n","Query 9 METEOR Score: 22.73\n","Query 10 METEOR Score: 13.76\n","Average METEOR Score: 27.07\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","\n","introduction = \"Generate an ending of the given story.\"\n","run(train_samples_sg, test_data_sg, 0, generations_23, queries_23, reference_endings_23,\n","    introduction=introduction, task_name=\"2_3_intro\")"]},{"cell_type":"markdown","id":"2cd95e36","metadata":{"id":"2cd95e36"},"source":["**Question:** Does overall task instruction help improve the model's 0-shot story ending generation quality?"]},{"cell_type":"markdown","id":"5ede0c13","metadata":{"id":"5ede0c13"},"source":["**Reference Answer:** According to the automatic evaluations, yes, but similar to 2.2, you may need further comparisons between the model generations in 2.1 and 2.3 to verify this conclusion."]},{"cell_type":"markdown","id":"a314a4db","metadata":{"id":"a314a4db"},"source":["You can print the saved caches and make more comparisons between the model generations in 2.1 and 2.3."]},{"cell_type":"code","execution_count":null,"id":"75485c15","metadata":{"id":"75485c15","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681741590062,"user_tz":-120,"elapsed":4,"user":{"displayName":"Silin Gao","userId":"06208253100379244126"}},"outputId":"0688a90b-d4b8-4edb-e70b-0f49656bbe6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Queries:\n","A few days ago I decided to take my dog Sable for a walk.\n","She is a half-pit bull half-bulldog with a lot of strength.\n","After I got her leash on I opened the garage to head outside.\n","She tried bolting out of the garage and dragged me along with her.\n","Output:\n","GPT-3.5 Generation:\n","I stumbled and fell to the ground, scraping my knee and elbow in the process. Sable didn\n","Reference:\n","After the half hour walk my muscles were sore as if I had worked out.\n"]}],"source":["print(\"Queries:\\n\"+queries_23[0])\n","print(\"GPT-3.5 Generation:\\n\"+generations_23[0])\n","print(\"Reference:\\n\"+reference_endings_23[0])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}