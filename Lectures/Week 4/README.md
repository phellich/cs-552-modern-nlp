
## Reading References for Week 4

| Lecture | Readings                   |
|:-------:|:---------------------------|
| 10      | [1] A. Vaswani et al., Attention Is All You Need. arXiv, 2017. [Online]. Available: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762) <br />[2] A. Rush, “The Annotated Transformer,” in Proceedings of Workshop for NLP Open Source Software (NLP-OSS), Jul. 2018, pp. 52–60. [Online]. Available: [https://aclanthology.org/W18-2509](https://aclanthology.org/W18-2509) <br />[3] J. Alammar, The illustrated transformer. 2017. [Online]. Available: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/) |
| 11      | |
| 12      | GPT: [1] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” 2018, [Online]. Available: [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) <br />GPT2: [2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language Models are Unsupervised Multitask Learners,” 2018, [Online]. Available: [https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)  |
