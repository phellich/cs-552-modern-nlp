
## Reading References for Week 5

| Lecture | Readings                   |
|:-------:|:---------------------------|
| 13      | Elmo: [1] M. E. Peters et al., “Deep Contextualized Word Representations,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Jun. 2018, pp. 2227–2237. [Online]. Available: [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202) <br />BERT: [2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jun. 2019, pp. 4171–4186. [Online]. Available: [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423) <br /> RoBERTa: [3] Y. Liu et al., RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv, 2019. [Online]. Available: [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692) <br />ELECTRA: [4] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In 8th International Conference on Learning Representations, ICLR 2020, OpenReview.net. Retrieved from [https://openreview.net/forum?id=r1xMH1BtvB](https://openreview.net/forum?id=r1xMH1BtvB) |
| 14      | [1] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf, “Transfer Learning in Natural Language Processing,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials, 2019, pp. 15–18. [Online]. Available: [https://www.ruder.io/state-of-transfer-learning-in-nlp/](https://www.ruder.io/state-of-transfer-learning-in-nlp/) |
| 15      | T5: [1] C. Raffel et al., Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv, 2019. arXiv, 2019. [Online]. Available: [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683) <br />BART: [2] M. Lewis et al., BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. arXiv, 2019. [Online]. Available: https://arxiv.org/abs/1910.13461  |
